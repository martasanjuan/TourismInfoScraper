{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "##import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, user_agent='msc', num_retries=2):\n",
    "    \"\"\"Download function that includes user agent support\"\"\"\n",
    "    print ('Downloading:', url)\n",
    "    headers = {'User-agent': user_agent}\n",
    "    request = urllib.request.Request(url, headers=headers)\n",
    "    try:\n",
    "        html = urllib.request.urlopen(request).read()\n",
    "    except urllib.request.URLError as e:\n",
    "        print ('Downloading:', e.reason)\n",
    "        html = None\n",
    "        if num_retries > 0:\n",
    "            if hasattr(e, 'code') and 500 <= e.code < 600:\n",
    "                # retry 5XX HTTP errors\n",
    "                html = download(url, user_agent, num_retries-1)\n",
    "    return html\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration():\n",
    "    max_errors = 5 # maximum number of consecutive download errors allowed\n",
    "    num_errors = 0 # current number of consecutive download errors\n",
    "    for page in itertools.count(1):\n",
    "        url = 'http://example.webscraping.com/view/-{}'.format(page)\n",
    "        html = download(url)\n",
    "        if html is None:\n",
    "            # received an error trying to download this webpage\n",
    "            num_errors += 1\n",
    "            if num_errors == max_errors:\n",
    "                # reached maximum amount of errors in a row so exit\n",
    "                break\n",
    "            # so assume have reached the last country ID and can stop downloading\n",
    "        else:\n",
    "            # success - can scrape the result\n",
    "            # ...\n",
    "            num_errors = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.robotparser as robotparser\n",
    "import urllib.parse as parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robots(url):\n",
    "    \"\"\"Initialize robots parser for this domain\n",
    "    \"\"\"\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    rp.set_url(parse.urljoin(url, '/robots.txt'))\n",
    "    rp.read()\n",
    "    return rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Throttle:\n",
    "    \"\"\"Throttle downloading by sleeping between requests to same domain\n",
    "    \"\"\"\n",
    "    def __init__(self, delay):\n",
    "        # amount of delay between downloads for each domain\n",
    "        self.delay = delay\n",
    "        # timestamp of when a domain was last accessed\n",
    "        self.domains = {}\n",
    "        \n",
    "    def wait(self, url):\n",
    "        domain = parse.urlparse(url).netloc\n",
    "        last_accessed = self.domains.get(domain)\n",
    "\n",
    "        if self.delay > 0 and last_accessed is not None:\n",
    "            sleep_secs = self.delay - (datetime.now() - last_accessed).seconds\n",
    "            if sleep_secs > 0:\n",
    "                time.sleep(sleep_secs)\n",
    "        self.domains[domain] = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(html):\n",
    "    \"\"\"Return a list of links from html \n",
    "    \"\"\"\n",
    "    # a regular expression to extract all links from the webpage\n",
    "    webpage_regex = re.compile('<a[^>]+href=[\"\\'](.*?)[\"\\']', re.IGNORECASE)\n",
    "    # list of all links from the webpage\n",
    "    #print(links)\n",
    "    #return links\n",
    "    htmltext = html.decode('utf-8')\n",
    "    return webpage_regex.findall(htmltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(seed_url, link):\n",
    "    \"\"\"Normalize this URL by removing hash and adding domain\n",
    "    \"\"\"\n",
    "    link, _ = parse.urldefrag(link) # remove hash to avoid duplicates\n",
    "    return parse.urljoin(seed_url, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_domain(url1, url2):\n",
    "    \"\"\"Return True if both URL's belong to same domain\n",
    "    \"\"\"\n",
    "    return urlparse(url1).netloc == urlparse(url2).netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_crawler(seed_url, link_regex=None, delay=5, max_depth=-1, max_urls=-1, headers=None, user_agent='Student', proxy=None, num_retries=1):\n",
    "    \"\"\"Crawl from the given seed URL following links matched by link_regex\n",
    "    \"\"\"\n",
    "    # the queue of URL's that still need to be crawled\n",
    "    crawl_queue = queue.deque([seed_url])\n",
    "    # the URL's that have been seen and at what depth\n",
    "    seen = {seed_url: 0}\n",
    "    # track how many URL's have been downloaded\n",
    "    num_urls = 0\n",
    "    rp = get_robots(seed_url)\n",
    "    throttle = Throttle(delay)\n",
    "    headers = headers or {}\n",
    "    if user_agent:\n",
    "        headers['User-agent'] = user_agent\n",
    "    \n",
    "    #Current directory where is located the script\n",
    "    \n",
    "    #currentDir = os.path.dirname(__file__)\n",
    "    #filename = \"TourismInfo.csv\"\n",
    "    #filePath = os.path.join(currentDir, filename)\n",
    "    \n",
    "    filePath = os.path.join(os.getcwd(), \"TourismInfo.csv\")\n",
    "    \n",
    "    ListaActividades = []\n",
    "    cabecera=[\"Titulo\", \"Idioma\", \"Duración\", \"Puntuación\"]\n",
    "    ListaActividades.append(cabecera)\n",
    "    \n",
    "    while crawl_queue:\n",
    "        url = crawl_queue.pop()\n",
    "\n",
    "        #print('url:', url)\n",
    "        # check url passes robots.txt restrictions\n",
    "        if rp.can_fetch(user_agent, url):\n",
    "            throttle.wait(url)\n",
    "                   \n",
    "            html = download(url, user_agent, num_retries)\n",
    "            \n",
    "            #print(url)\n",
    "            #if re.search(r'/s/\\b[\\D]', ciudad, url) and not re.search('opiniones', url):\n",
    "            if re.search(r'/sevilla/\\b[\\D]', url) and not re.search('opiniones', url): \n",
    "                #Se trata de una página que no es de enlace /número ni es de opiniones\n",
    "                ListaActividades.append(scrape(html))\n",
    "            \n",
    "            links = []\n",
    "\n",
    "            depth = seen[url]\n",
    "            if depth != max_depth:\n",
    "                # can still crawl further\n",
    "                if link_regex:\n",
    "                    # filter for links matching our regular expression\n",
    "                    links.extend(link for link in get_links(html) if re.search(link_regex, link)\n",
    "                                   and not re.search('opiniones', link))\n",
    "\n",
    "                for link in links:\n",
    "                    link = normalize(seed_url, link)\n",
    "                    # check whether already crawled this link\n",
    "                    if link not in seen:\n",
    "                        seen[link] = depth + 1\n",
    "                        # check link is within same domain\n",
    "                        if same_domain(seed_url, link):\n",
    "                            # success! add this new link to queue\n",
    "                            crawl_queue.append(link)\n",
    "\n",
    "            # check whether have reached downloaded maximum\n",
    "            num_urls += 1\n",
    "            if num_urls == max_urls:\n",
    "                break\n",
    "    \n",
    "        else:\n",
    "            print('Blocked by robots.txt:', url)\n",
    "        \n",
    "    #Procedemos a guardar el listado de actividades en un fichero CSV\n",
    "    with open(filePath, 'w', newline='') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        for act in ListaActividades:\n",
    "            writer.writerow(act)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue as queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    \n",
    "    #Titulo\n",
    "    try:\n",
    "        div = soup.find(attrs={'class':'a-title-activity'}) \n",
    "        titulo = div.text \n",
    "    except:\n",
    "        titulo = 'sin titulo'\n",
    "    \n",
    "    #Idioma\n",
    "    try:\n",
    "        div = soup.find(attrs={'m-activity-detail m-activity-detail--language'})\n",
    "        idioma = div.h3.img['alt']\n",
    "    except:\n",
    "        idioma = 'sin idioma'\n",
    "    \n",
    "    #Duración\n",
    "    try:\n",
    "        div = soup.find(attrs={'class':'m-activity-detail m-activity-detail--duration'}) \n",
    "        duracion = div.p.text  \n",
    "    except:\n",
    "        duracion = 'sin duracion'\n",
    "        \n",
    "    #Puntuación\n",
    "    try:\n",
    "        div = soup.find(attrs={'o-rating__title'})\n",
    "        punt = div.text\n",
    "    except:\n",
    "        punt = 'sin puntuacion'\n",
    "    \n",
    "    \n",
    "    actividad=[]\n",
    "    actividad = [titulo, idioma, duracion, punt]\n",
    "    \n",
    "    #print(actividad)\n",
    "    return actividad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    #html = download('https://www.civitatis.com/es/sevilla/excursion-tanger/')\n",
    "    #print (scrape(html))\n",
    "    #link_crawler('https://www.civitatis.com/es/sevilla/', 'es/sevilla/', delay=0, num_retries=1, max_depth=2, user_agent='StudentCrawler')\n",
    "    None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: https://www.civitatis.com/es/sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/2/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/entrada-acuario-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/ruta-bares-tapas-triana/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/paseo-caballo-andaluz/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/espectaculo-flamenco-triana/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-tapas-flamenco/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-segway-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-marismas-arroceras-guadalquivir/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/paseo-yate-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/alquiler-motos-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/visita-guiada-museo-bellas-artes/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-cofrade-semana-santa/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/visita-guiada-parque-maria-luisa/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-kayak-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-semana-santa-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-misterios-leyendas-triana/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-sierra-aracena/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-caminito-rey/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-gibraltar/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-privado-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-tanger/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-carmona/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/paseo-calesa-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-sevilla-paranormal/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/visita-guiada-catedral-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-donana/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-jerez-cadiz/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/visita-toros-bravos/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-italica/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-cordoba/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-bicicleta-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/visita-guiada-alcazar/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/espectaculo-flamenco-palacio-andaluz/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-pueblos-blancos-ronda/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/excursion-alhambra/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/paseo-barco-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-nocturno-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/visita-guiada-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/tour-sevilla/\n",
      "Downloading: https://www.civitatis.com/es/sevilla/traslados/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ciudad = 'sevilla'\n",
    "    #link_crawler(ciudad, 'https://www.civitatis.com/es/%s/' % ciudad, 'es/%s/' % ciudad, delay=0, num_retries=1, max_depth=1, user_agent='StudentCrawler')\n",
    "    link_crawler('https://www.civitatis.com/es/%s/' % ciudad, 'es/%s/' % ciudad, delay=0, num_retries=1, max_depth=4, user_agent='StudentCrawler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
